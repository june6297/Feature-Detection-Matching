{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"12ykYzAc4YIRa2CpQ1YtFm_YD3JgMfgo3","authorship_tag":"ABX9TyMNGNDFsDltbanA4TAoKH5P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_T2D6NZiuw4","executionInfo":{"status":"ok","timestamp":1725463158723,"user_tz":-540,"elapsed":70197,"user":{"displayName":"박세훈","userId":"03887704865760519772"}},"outputId":"4a942689-e0ee-49e4-834b-2a2d9b1fcc6a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating SIFT: 100%|██████████| 59/59 [00:08<00:00,  6.79it/s]\n","Evaluating ORB: 100%|██████████| 59/59 [00:04<00:00, 11.86it/s]\n","Evaluating AKAZE: 100%|██████████| 59/59 [00:05<00:00, 10.28it/s]\n","Evaluating BRISK: 100%|██████████| 59/59 [00:50<00:00,  1.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Extended Results:\n","SIFT:\n","  Quality = 0.0153\n","  Repeatability = 0.1033\n","  Avg Keypoints = 341.44\n","  Avg Matches = 93.05\n","  Avg Match Distance = 325.71\n","  Avg Good Matches = 4.32\n","\n","ORB:\n","  Quality = 0.0166\n","  Repeatability = 0.0633\n","  Avg Keypoints = 739.31\n","  Avg Matches = 194.44\n","  Avg Match Distance = 66.11\n","  Avg Good Matches = 11.46\n","\n","AKAZE:\n","  Quality = 0.0092\n","  Repeatability = 0.1892\n","  Avg Keypoints = 151.37\n","  Avg Matches = 44.36\n","  Avg Match Distance = 637.31\n","  Avg Good Matches = 0.97\n","\n","BRISK:\n","  Quality = 0.0038\n","  Repeatability = 0.0390\n","  Avg Keypoints = 2053.53\n","  Avg Matches = 530.36\n","  Avg Match Distance = 646.44\n","  Avg Good Matches = 3.76\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","\n","def detect_features(image, algorithm):\n","    if algorithm == 'SIFT':\n","        detector = cv2.SIFT_create()\n","    elif algorithm == 'ORB':\n","        detector = cv2.ORB_create(nfeatures=1000)\n","    elif algorithm == 'AKAZE':\n","        detector = cv2.AKAZE_create()\n","    elif algorithm == 'BRISK':\n","        detector = cv2.BRISK_create()\n","    else:\n","        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n","\n","    keypoints, descriptors = detector.detectAndCompute(image, None)\n","    return keypoints, descriptors\n","\n","def match_features(descriptors1, descriptors2, algorithm):\n","    if descriptors1 is None or descriptors2 is None:\n","        return []\n","\n","    if algorithm in ['SIFT', 'AKAZE', 'BRISK']:\n","        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","    else:  # ORB\n","        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","    matches = bf.match(descriptors1, descriptors2)\n","    return sorted(matches, key=lambda x: x.distance)\n","\n","def evaluate_feature_quality(matches, num_keypoints):\n","    if not matches or num_keypoints == 0:\n","        return 0, 0, 0\n","\n","    distances = [m.distance for m in matches]\n","    avg_distance = np.mean(distances)\n","    min_distance = np.min(distances)\n","    max_distance = np.max(distances)\n","\n","    # Adjust the threshold for \"good\" matches\n","    good_matches = [m for m in matches if m.distance < 0.8 * avg_distance]\n","    quality = len(good_matches) / num_keypoints\n","\n","    return quality, avg_distance, len(good_matches)\n","\n","def evaluate_repeatability(keypoints1, keypoints2, matches, shape):\n","    if len(matches) < 4:\n","        return 0\n","\n","    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n","    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n","\n","    try:\n","        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","\n","        if H is None:\n","            return 0\n","\n","        h, w = shape\n","        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n","        dst = cv2.perspectiveTransform(pts, H)\n","\n","        good_matches = mask.ravel() == 1\n","        return np.sum(good_matches) / len(matches)\n","\n","    except cv2.error:\n","        return 0\n","\n","def evaluate_algorithm_extended(algorithm, image_folder):\n","    image_files = sorted(os.listdir(image_folder))\n","    quality_scores = []\n","    repeatability_scores = []\n","    total_keypoints = []\n","    match_counts = []\n","    avg_distances = []\n","    good_match_counts = []\n","\n","    for i in tqdm(range(len(image_files) - 1), desc=f\"Evaluating {algorithm}\"):\n","        image1_path = os.path.join(image_folder, image_files[i])\n","        image2_path = os.path.join(image_folder, image_files[i+1])\n","\n","        image1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)\n","        image2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if image1 is None or image2 is None:\n","            print(f\"Warning: Unable to read image {image_files[i]} or {image_files[i+1]}\")\n","            continue\n","\n","        keypoints1, descriptors1 = detect_features(image1, algorithm)\n","        keypoints2, descriptors2 = detect_features(image2, algorithm)\n","\n","        total_keypoints.append(len(keypoints1))\n","\n","        matches = match_features(descriptors1, descriptors2, algorithm)\n","        match_counts.append(len(matches))\n","\n","        quality_score, avg_distance, good_match_count = evaluate_feature_quality(matches, len(keypoints1))\n","        quality_scores.append(quality_score)\n","        avg_distances.append(avg_distance)\n","        good_match_counts.append(good_match_count)\n","\n","        repeatability_score = evaluate_repeatability(keypoints1, keypoints2, matches, image1.shape)\n","        repeatability_scores.append(repeatability_score)\n","\n","    avg_keypoints = np.mean(total_keypoints) if total_keypoints else 0\n","    avg_matches = np.mean(match_counts) if match_counts else 0\n","    avg_quality = np.mean(quality_scores) if quality_scores else 0\n","    avg_repeatability = np.mean(repeatability_scores) if repeatability_scores else 0\n","    avg_distance = np.mean(avg_distances) if avg_distances else 0\n","    avg_good_matches = np.mean(good_match_counts) if good_match_counts else 0\n","\n","    return avg_quality, avg_repeatability, avg_keypoints, avg_matches, avg_distance, avg_good_matches\n","\n","# 메인 실행 부분\n","image_folder = \"/content/drive/MyDrive/KeyPoint/dataset_split/test/damaged\"\n","algorithms = ['SIFT', 'ORB', 'AKAZE', 'BRISK']\n","\n","extended_results = {}\n","\n","for algorithm in algorithms:\n","    quality, repeatability, avg_keypoints, avg_matches, avg_distance, avg_good_matches = evaluate_algorithm_extended(algorithm, image_folder)\n","    extended_results[algorithm] = {\n","        'quality': quality,\n","        'repeatability': repeatability,\n","        'avg_keypoints': avg_keypoints,\n","        'avg_matches': avg_matches,\n","        'avg_distance': avg_distance,\n","        'avg_good_matches': avg_good_matches\n","    }\n","\n","print(\"Extended Results:\")\n","for alg, res in extended_results.items():\n","    print(f\"{alg}:\")\n","    print(f\"  Quality = {res['quality']:.4f}\")\n","    print(f\"  Repeatability = {res['repeatability']:.4f}\")\n","    print(f\"  Avg Keypoints = {res['avg_keypoints']:.2f}\")\n","    print(f\"  Avg Matches = {res['avg_matches']:.2f}\")\n","    print(f\"  Avg Match Distance = {res['avg_distance']:.2f}\")\n","    print(f\"  Avg Good Matches = {res['avg_good_matches']:.2f}\")\n","    print()"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","\n","def detect_features(image, algorithm):\n","    if algorithm == 'SIFT':\n","        detector = cv2.SIFT_create(nfeatures=1000)  # 특징점 수 증가\n","    elif algorithm == 'ORB':\n","        detector = cv2.ORB_create(nfeatures=1500, scaleFactor=1.2, nlevels=8)  # 파라미터 조정\n","    elif algorithm == 'AKAZE':\n","        detector = cv2.AKAZE_create(threshold=0.001)  # 임계값 낮춤\n","    elif algorithm == 'BRISK':\n","        detector = cv2.BRISK_create(thresh=10, octaves=4)  # 파라미터 조정\n","    else:\n","        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n","\n","    keypoints, descriptors = detector.detectAndCompute(image, None)\n","    return keypoints, descriptors\n","\n","def match_features(descriptors1, descriptors2, algorithm):\n","    if descriptors1 is None or descriptors2 is None:\n","        return []\n","\n","    if algorithm in ['SIFT', 'AKAZE', 'BRISK']:\n","        bf = cv2.BFMatcher(cv2.NORM_L2)\n","    else:  # ORB\n","        bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n","\n","    # 2-nn 매칭 사용\n","    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n","    good_matches = []\n","    for m, n in matches:\n","        if m.distance < 0.75 * n.distance:  # Lowe's ratio test\n","            good_matches.append(m)\n","\n","    return good_matches\n","\n","def evaluate_feature_quality(matches, num_keypoints):\n","    if not matches or num_keypoints == 0:\n","        return 0, 0, 0\n","\n","    distances = [m.distance for m in matches]\n","    avg_distance = np.mean(distances)\n","\n","    # Quality 계산 방식 변경\n","    quality = len(matches) / num_keypoints\n","\n","    return quality, avg_distance, len(matches)\n","\n","def evaluate_repeatability(keypoints1, keypoints2, matches, shape):\n","    if len(matches) < 4:\n","        return 0\n","\n","    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n","    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n","\n","    try:\n","        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","\n","        if H is None:\n","            return 0\n","\n","        h, w = shape\n","        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n","        dst = cv2.perspectiveTransform(pts, H)\n","\n","        good_matches = mask.ravel() == 1\n","        return np.sum(good_matches) / len(matches)\n","\n","    except cv2.error:\n","        return 0\n","\n","def preprocess_image(image):\n","    # 이미지 전처리: 대비 향상\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    return clahe.apply(image)\n","\n","def evaluate_algorithm_extended(algorithm, image_folder):\n","    image_files = sorted(os.listdir(image_folder))\n","    quality_scores = []\n","    repeatability_scores = []\n","    total_keypoints = []\n","    match_counts = []\n","    avg_distances = []\n","\n","    for i in tqdm(range(len(image_files) - 1), desc=f\"Evaluating {algorithm}\"):\n","        image1_path = os.path.join(image_folder, image_files[i])\n","        image2_path = os.path.join(image_folder, image_files[i+1])\n","\n","        image1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)\n","        image2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if image1 is None or image2 is None:\n","            print(f\"Warning: Unable to read image {image_files[i]} or {image_files[i+1]}\")\n","            continue\n","\n","        # 이미지 전처리 적용\n","        image1 = preprocess_image(image1)\n","        image2 = preprocess_image(image2)\n","\n","        keypoints1, descriptors1 = detect_features(image1, algorithm)\n","        keypoints2, descriptors2 = detect_features(image2, algorithm)\n","\n","        total_keypoints.append(len(keypoints1))\n","\n","        matches = match_features(descriptors1, descriptors2, algorithm)\n","        match_counts.append(len(matches))\n","\n","        quality_score, avg_distance, good_match_count = evaluate_feature_quality(matches, len(keypoints1))\n","        quality_scores.append(quality_score)\n","        avg_distances.append(avg_distance)\n","\n","        repeatability_score = evaluate_repeatability(keypoints1, keypoints2, matches, image1.shape)\n","        repeatability_scores.append(repeatability_score)\n","\n","    avg_keypoints = np.mean(total_keypoints) if total_keypoints else 0\n","    avg_matches = np.mean(match_counts) if match_counts else 0\n","    avg_quality = np.mean(quality_scores) if quality_scores else 0\n","    avg_repeatability = np.mean(repeatability_scores) if repeatability_scores else 0\n","    avg_distance = np.mean(avg_distances) if avg_distances else 0\n","\n","    return avg_quality, avg_repeatability, avg_keypoints, avg_matches, avg_distance\n","\n","# 메인 실행 부분\n","image_folder = \"/content/drive/MyDrive/KeyPoint/dataset_split/test/damaged\"\n","algorithms = ['SIFT', 'ORB', 'AKAZE', 'BRISK']\n","\n","extended_results = {}\n","\n","for algorithm in algorithms:\n","    quality, repeatability, avg_keypoints, avg_matches, avg_distance = evaluate_algorithm_extended(algorithm, image_folder)\n","    extended_results[algorithm] = {\n","        'quality': quality,\n","        'repeatability': repeatability,\n","        'avg_keypoints': avg_keypoints,\n","        'avg_matches': avg_matches,\n","        'avg_distance': avg_distance\n","    }\n","\n","print(\"Extended Results:\")\n","for alg, res in extended_results.items():\n","    print(f\"{alg}:\")\n","    print(f\"  Quality = {res['quality']:.4f}\")\n","    print(f\"  Repeatability = {res['repeatability']:.4f}\")\n","    print(f\"  Avg Keypoints = {res['avg_keypoints']:.2f}\")\n","    print(f\"  Avg Matches = {res['avg_matches']:.2f}\")\n","    print(f\"  Avg Match Distance = {res['avg_distance']:.2f}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5yGLMFfkj5a","executionInfo":{"status":"ok","timestamp":1725463440992,"user_tz":-540,"elapsed":126251,"user":{"displayName":"박세훈","userId":"03887704865760519772"}},"outputId":"e5a023c7-d367-429f-9b18-33104a0708d0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating SIFT: 100%|██████████| 59/59 [00:06<00:00,  9.75it/s]\n","Evaluating ORB: 100%|██████████| 59/59 [00:03<00:00, 15.17it/s]\n","Evaluating AKAZE: 100%|██████████| 59/59 [00:04<00:00, 14.04it/s]\n","Evaluating BRISK: 100%|██████████| 59/59 [01:51<00:00,  1.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Extended Results:\n","SIFT:\n","  Quality = 0.0058\n","  Repeatability = 0.1546\n","  Avg Keypoints = 496.73\n","  Avg Matches = 2.61\n","  Avg Match Distance = 142.13\n","\n","ORB:\n","  Quality = 0.0039\n","  Repeatability = 0.4502\n","  Avg Keypoints = 1186.39\n","  Avg Matches = 4.46\n","  Avg Match Distance = 47.85\n","\n","AKAZE:\n","  Quality = 0.0026\n","  Repeatability = 0.0000\n","  Avg Keypoints = 336.54\n","  Avg Matches = 0.85\n","  Avg Match Distance = 256.59\n","\n","BRISK:\n","  Quality = 0.0010\n","  Repeatability = 0.1978\n","  Avg Keypoints = 4896.76\n","  Avg Matches = 3.41\n","  Avg Match Distance = 360.74\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","\n","def detect_features(image, algorithm):\n","    if algorithm == 'SIFT':\n","        detector = cv2.SIFT_create(nfeatures=500, contrastThreshold=0.04)\n","    elif algorithm == 'ORB':\n","        detector = cv2.ORB_create(nfeatures=1000, scaleFactor=1.2, nlevels=8)\n","    elif algorithm == 'AKAZE':\n","        detector = cv2.AKAZE_create(threshold=0.001)\n","    elif algorithm == 'BRISK':\n","        detector = cv2.BRISK_create(thresh=30, octaves=3)\n","    else:\n","        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n","\n","    keypoints, descriptors = detector.detectAndCompute(image, None)\n","    return keypoints, descriptors\n","\n","def match_features(descriptors1, descriptors2, algorithm):\n","    if descriptors1 is None or descriptors2 is None:\n","        return []\n","\n","    if algorithm in ['SIFT', 'AKAZE', 'BRISK']:\n","        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","    else:  # ORB\n","        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","    matches = bf.match(descriptors1, descriptors2)\n","    matches = sorted(matches, key=lambda x: x.distance)\n","\n","    # 상위 75%의 매치만 사용\n","    good_matches = matches[:int(len(matches) * 0.75)]\n","\n","    return good_matches\n","\n","def evaluate_feature_quality(matches, num_keypoints):\n","    if not matches or num_keypoints == 0:\n","        return 0, 0, 0\n","\n","    distances = [m.distance for m in matches]\n","    avg_distance = np.mean(distances)\n","\n","    # Quality 계산 방식 변경\n","    quality = len(matches) / num_keypoints\n","\n","    return quality, avg_distance, len(matches)\n","\n","def evaluate_repeatability(keypoints1, keypoints2, matches, shape):\n","    if len(matches) < 4:\n","        return 0\n","\n","    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n","    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n","\n","    try:\n","        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","\n","        if H is None:\n","            return 0\n","\n","        h, w = shape\n","        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n","        dst = cv2.perspectiveTransform(pts, H)\n","\n","        good_matches = mask.ravel() == 1\n","        return np.sum(good_matches) / len(matches)\n","\n","    except cv2.error:\n","        return 0\n","\n","def preprocess_image(image):\n","    # 가우시안 블러 적용\n","    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n","    # 샤프닝\n","    sharpened = cv2.addWeighted(image, 1.5, blurred, -0.5, 0)\n","    return sharpened\n","\n","def evaluate_algorithm_extended(algorithm, image_folder):\n","    image_files = sorted(os.listdir(image_folder))\n","    quality_scores = []\n","    repeatability_scores = []\n","    total_keypoints = []\n","    match_counts = []\n","    avg_distances = []\n","\n","    for i in tqdm(range(len(image_files) - 1), desc=f\"Evaluating {algorithm}\"):\n","        image1_path = os.path.join(image_folder, image_files[i])\n","        image2_path = os.path.join(image_folder, image_files[i+1])\n","\n","        image1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)\n","        image2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)\n","\n","        if image1 is None or image2 is None:\n","            print(f\"Warning: Unable to read image {image_files[i]} or {image_files[i+1]}\")\n","            continue\n","\n","        # 이미지 전처리 적용\n","        image1 = preprocess_image(image1)\n","        image2 = preprocess_image(image2)\n","\n","        keypoints1, descriptors1 = detect_features(image1, algorithm)\n","        keypoints2, descriptors2 = detect_features(image2, algorithm)\n","\n","        total_keypoints.append(len(keypoints1))\n","\n","        matches = match_features(descriptors1, descriptors2, algorithm)\n","        match_counts.append(len(matches))\n","\n","        quality_score, avg_distance, good_match_count = evaluate_feature_quality(matches, len(keypoints1))\n","        quality_scores.append(quality_score)\n","        avg_distances.append(avg_distance)\n","\n","        repeatability_score = evaluate_repeatability(keypoints1, keypoints2, matches, image1.shape)\n","        repeatability_scores.append(repeatability_score)\n","\n","    avg_keypoints = np.mean(total_keypoints) if total_keypoints else 0\n","    avg_matches = np.mean(match_counts) if match_counts else 0\n","    avg_quality = np.mean(quality_scores) if quality_scores else 0\n","    avg_repeatability = np.mean(repeatability_scores) if repeatability_scores else 0\n","    avg_distance = np.mean(avg_distances) if avg_distances else 0\n","\n","    return avg_quality, avg_repeatability, avg_keypoints, avg_matches, avg_distance\n","\n","# 메인 실행 부분\n","image_folder = \"/content/drive/MyDrive/KeyPoint/dataset_split/test/damaged\"\n","algorithms = ['SIFT', 'ORB', 'AKAZE', 'BRISK']\n","\n","extended_results = {}\n","\n","for algorithm in algorithms:\n","    quality, repeatability, avg_keypoints, avg_matches, avg_distance = evaluate_algorithm_extended(algorithm, image_folder)\n","    extended_results[algorithm] = {\n","        'quality': quality,\n","        'repeatability': repeatability,\n","        'avg_keypoints': avg_keypoints,\n","        'avg_matches': avg_matches,\n","        'avg_distance': avg_distance\n","    }\n","\n","print(\"Extended Results:\")\n","for alg, res in extended_results.items():\n","    print(f\"{alg}:\")\n","    print(f\"  Quality = {res['quality']:.4f}\")\n","    print(f\"  Repeatability = {res['repeatability']:.4f}\")\n","    print(f\"  Avg Keypoints = {res['avg_keypoints']:.2f}\")\n","    print(f\"  Avg Matches = {res['avg_matches']:.2f}\")\n","    print(f\"  Avg Match Distance = {res['avg_distance']:.2f}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiPLTDsHoL-I","executionInfo":{"status":"ok","timestamp":1725463610315,"user_tz":-540,"elapsed":87267,"user":{"displayName":"박세훈","userId":"03887704865760519772"}},"outputId":"21b62d89-14b6-43e4-dec0-73d6af26d5ef"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating SIFT: 100%|██████████| 59/59 [00:08<00:00,  6.63it/s]\n","Evaluating ORB: 100%|██████████| 59/59 [00:05<00:00, 11.57it/s]\n","Evaluating AKAZE: 100%|██████████| 59/59 [00:05<00:00, 10.29it/s]\n","Evaluating BRISK: 100%|██████████| 59/59 [01:07<00:00,  1.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Extended Results:\n","SIFT:\n","  Quality = 0.2130\n","  Repeatability = 0.1480\n","  Avg Keypoints = 283.42\n","  Avg Matches = 56.42\n","  Avg Match Distance = 313.58\n","\n","ORB:\n","  Quality = 0.1967\n","  Repeatability = 0.0744\n","  Avg Keypoints = 770.39\n","  Avg Matches = 151.68\n","  Avg Match Distance = 63.36\n","\n","AKAZE:\n","  Quality = 0.2481\n","  Repeatability = 0.2063\n","  Avg Keypoints = 180.76\n","  Avg Matches = 38.97\n","  Avg Match Distance = 603.13\n","\n","BRISK:\n","  Quality = 0.2384\n","  Repeatability = 0.0361\n","  Avg Keypoints = 2514.49\n","  Avg Matches = 495.34\n","  Avg Match Distance = 625.42\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","def detect_features(image, algorithm):\n","    if algorithm == 'SIFT':\n","        detector = cv2.SIFT_create(nfeatures=500, contrastThreshold=0.04)\n","    elif algorithm == 'ORB':\n","        detector = cv2.ORB_create(nfeatures=1000, scaleFactor=1.2, nlevels=8)\n","    elif algorithm == 'AKAZE':\n","        detector = cv2.AKAZE_create(threshold=0.001)\n","    elif algorithm == 'BRISK':\n","        detector = cv2.BRISK_create(thresh=30, octaves=3)\n","    else:\n","        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n","\n","    keypoints, descriptors = detector.detectAndCompute(image, None)\n","    return keypoints, descriptors\n","\n","def match_features(descriptors1, descriptors2, algorithm):\n","    if descriptors1 is None or descriptors2 is None:\n","        return []\n","\n","    if algorithm in ['SIFT', 'AKAZE', 'BRISK']:\n","        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","    else:  # ORB\n","        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","    matches = bf.match(descriptors1, descriptors2)\n","    matches = sorted(matches, key=lambda x: x.distance)\n","\n","    # 상위 75%의 매치만 사용\n","    good_matches = matches[:int(len(matches) * 0.75)]\n","\n","    return good_matches\n","\n","def preprocess_image(image):\n","    # 가우시안 블러 적용\n","    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n","    # 샤프닝\n","    sharpened = cv2.addWeighted(image, 1.5, blurred, -0.5, 0)\n","    return sharpened\n","\n","def visualize_features_and_matches(image1, image2, keypoints1, keypoints2, matches, algorithm):\n","    # 특징점 시각화\n","    img1_keypoints = cv2.drawKeypoints(image1, keypoints1, None, color=(0, 255, 0))\n","    img2_keypoints = cv2.drawKeypoints(image2, keypoints2, None, color=(0, 255, 0))\n","\n","    # 매칭 결과 시각화\n","    img_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches, None,\n","                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","    # 결과 출력\n","    plt.figure(figsize=(20, 10))\n","    plt.subplot(221), plt.imshow(cv2.cvtColor(img1_keypoints, cv2.COLOR_BGR2RGB))\n","    plt.title(f'{algorithm} Keypoints - Image 1'), plt.axis('off')\n","    plt.subplot(222), plt.imshow(cv2.cvtColor(img2_keypoints, cv2.COLOR_BGR2RGB))\n","    plt.title(f'{algorithm} Keypoints - Image 2'), plt.axis('off')\n","    plt.subplot(212), plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n","    plt.title(f'{algorithm} Matches'), plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def process_image_pair(image1_path, image2_path, algorithm):\n","    # 이미지 읽기\n","    image1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)\n","    image2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)\n","\n","    if image1 is None or image2 is None:\n","        print(f\"Warning: Unable to read image {image1_path} or {image2_path}\")\n","        return\n","\n","    # 이미지 전처리\n","    image1 = preprocess_image(image1)\n","    image2 = preprocess_image(image2)\n","\n","    # 특징점 검출\n","    keypoints1, descriptors1 = detect_features(image1, algorithm)\n","    keypoints2, descriptors2 = detect_features(image2, algorithm)\n","\n","    # 특징점 매칭\n","    matches = match_features(descriptors1, descriptors2, algorithm)\n","\n","    # 결과 시각화\n","    visualize_features_and_matches(image1, image2, keypoints1, keypoints2, matches, algorithm)\n","\n","    return len(keypoints1), len(keypoints2), len(matches)\n","\n","# 메인 실행 부분\n","image_folder = \"/content/drive/MyDrive/KeyPoint/dataset_split/test/damaged\"\n","algorithms = ['SIFT', 'ORB', 'AKAZE', 'BRISK']\n","\n","# 첫 두 개의 이미지에 대해 모든 알고리즘 실행\n","image_files = sorted(os.listdir(image_folder))\n","image1_path = os.path.join(image_folder, image_files[0])\n","image2_path = os.path.join(image_folder, image_files[1])\n","\n","for algorithm in algorithms:\n","    print(f\"\\nProcessing {algorithm}:\")\n","    num_keypoints1, num_keypoints2, num_matches = process_image_pair(image1_path, image2_path, algorithm)\n","    print(f\"Number of Keypoints in Image 1: {num_keypoints1}\")\n","    print(f\"Number of Keypoints in Image 2: {num_keypoints2}\")\n","    print(f\"Number of Matches: {num_matches}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16ePdoadhKlBYpGyv7xQg8LEF8ZiQZtJn"},"id":"hrL0jsAmo-1V","executionInfo":{"status":"ok","timestamp":1725463747035,"user_tz":-540,"elapsed":11417,"user":{"displayName":"박세훈","userId":"03887704865760519772"}},"outputId":"840dd124-5f14-499c-9f0c-06d23ab36541"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["각 알고리즘의 성능을 분석\n","\n","1. SIFT:\n","   - 중간 정도의 특징점 수를 검출합니다.\n","   - 두 이미지 간 특징점 수의 차이가 있습니다 (240 vs 404).\n","   - 60개의 매치를 찾았습니다. 첫 번째 이미지의 특징점 중 25%가 매치되었습니다.\n","\n","2. ORB:\n","   - SIFT보다 더 많은 특징점을 검출합니다.\n","   - 두 이미지에서 비슷한 수의 특징점을 찾았습니다 (782 vs 868).\n","   - 128개의 매치로, 가장 많은 매치를 찾았습니다. 첫 번째 이미지의 특징점 중 약 16.4%가 매치되었습니다.\n","\n","3. AKAZE:\n","   - 가장 적은 수의 특징점을 검출합니다.\n","   - 두 이미지 간 특징점 수의 차이가 큽니다 (79 vs 238).\n","   - 20개의 매치로, 가장 적은 매치를 찾았습니다. 하지만 첫 번째 이미지의 특징점 중 약 25.3%가 매치되어 SIFT와 비슷한 비율을 보입니다.\n","\n","4. BRISK:\n","   - 가장 많은 특징점을 검출합니다.\n","   - 두 이미지에서 모두 3000개 이상의 특징점을 찾았습니다.\n","   - 759개의 매치로, 절대적인 수치로는 가장 많은 매치를 찾았습니다. 하지만 첫 번째 이미지의 특징점 중 약 21.2%가 매치되어 비율로는 중간 정도입니다.\n","\n","분석:\n","\n","1. 특징점 검출 능력:\n","   BRISK > ORB > SIFT > AKAZE\n","   BRISK가 압도적으로 많은 특징점을 찾지만, 이것이 항상 좋은 것은 아닙니다. 처리 시간이 길어질 수 있습니다.\n","\n","2. 매칭 수:\n","   BRISK > ORB > SIFT > AKAZE\n","   절대적인 매치 수에서는 BRISK가 가장 뛰어납니다.\n","\n","3. 매칭 비율 (첫 번째 이미지의 특징점 대비):\n","   AKAZE (25.3%) ≈ SIFT (25%) > BRISK (21.2%) > ORB (16.4%)\n","   AKAZE와 SIFT가 가장 높은 매칭 비율을 보입니다.\n","\n","4. 일관성:\n","   ORB가 두 이미지에서 가장 일관된 수의 특징점을 찾았습니다.\n","\n","결론:\n","1. 정확성이 가장 중요하다면 AKAZE나 SIFT를 선택하는 것이 좋습니다. 이들은 높은 매칭 비율을 보입니다.\n","2. 많은 특징점과 매치가 필요하다면 BRISK가 좋은 선택일 수 있습니다.\n","3. 속도와 일관성이 중요하다면 ORB를 고려해볼 만합니다.\n"],"metadata":{"id":"Hled21M9qLS5"}},{"cell_type":"code","source":[],"metadata":{"id":"MmqwKS-lqZIO"},"execution_count":null,"outputs":[]}]}